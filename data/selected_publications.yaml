publications:
  - title: "The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio"
    authors: "Sicong Leng, Yun Xing, Zesen Cheng, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, Lidong Bing"
    venue: "NeurIPS 2025 Datasets and Benchmark Track"
    paper_url: "https://arxiv.org/abs/2410.12787"
    code_url: "https://github.com/DAMO-NLP-SG/CMM"
    preview_image: "assets/img/publication_preview_1.jpg"
    summary: |
      LLaVA-OneVision-1.5 is a fully open-source multimodal training framework that democratizes the development of large multimodal models (LMMs). This work represents a significant step towards making state-of-the-art multimodal AI accessible to the broader research community. The framework provides comprehensive tools and pipelines for training, fine-tuning, and deploying multimodal models that can understand both visual and textual information. By open-sourcing the complete training infrastructure, data processing pipelines, and model weights, LLaVA-OneVision-1.5 enables researchers and practitioners to build upon cutting-edge multimodal AI without requiring massive computational resources or proprietary datasets. The framework supports various vision-language tasks including image captioning, visual question answering, and multimodal reasoning, making it a versatile platform for advancing multimodal AI research.

  - title: "Mitigating Object Hallucination via Concentric Causal Attention"
    authors: "Yun Xing, Yiheng Li, Ivan Laptev, Shijian Lu"
    venue: "NeurIPS 2024"
    paper_url: "https://arxiv.org/abs/2410.15926"
    code_url: "https://github.com/xing0047/cca-llava"
    preview_image: "assets/img/publication_preview_2.jpg"
    summary: |
      UniME-V2 introduces an innovative approach to multimodal embedding learning by leveraging Multimodal Large Language Models (MLLMs) as judges to evaluate and guide the learning process. This work addresses the fundamental challenge of learning universal embeddings that can effectively represent diverse multimodal data including images, text, and their combinations. The key innovation is using MLLMs to assess the quality of embeddings through various evaluation criteria, enabling more nuanced and context-aware learning. This "MLLM-as-a-Judge" paradigm allows the embedding model to learn from rich feedback signals that go beyond simple similarity metrics. The framework achieves superior performance across multiple multimodal benchmarks, demonstrating the effectiveness of using advanced language models to guide representation learning. UniME-V2's approach opens new directions for developing more robust and generalizable multimodal embeddings that can better capture semantic relationships across different modalities.

  - title: "Domain Adaptive Video Segmentation via Temporal Pseudo Supervision"
    authors: "Yun Xing, Dayan Guan, Jiaxing Huang, Shijian Lu"
    venue: "ECCV 2022"
    paper_url: "https://arxiv.org/abs/2207.02372"
    code_url: "https://github.com/xing0047/TPS"
    preview_image: "assets/img/publication_preview_3.jpg"
    summary: |
      This ICCV 2025 Highlight paper presents a novel approach to self-supervised visual representation learning by introducing region-based cluster discrimination. As the project leader, I directed the development of this method which addresses a key limitation in existing contrastive learning approaches: their inability to effectively model fine-grained spatial relationships within images. The proposed framework performs clustering at the region level rather than the global image level, enabling the model to learn more discriminative local features while maintaining global semantic coherence. By discriminating between clusters of image regions, the method captures both local patterns and their compositional relationships, leading to more robust and transferable visual representations. The approach achieves state-of-the-art performance on multiple downstream tasks including image classification, object detection, and instance segmentation. This work demonstrates that region-level discrimination is a powerful paradigm for learning visual representations that are both semantically meaningful and spatially aware, making them particularly effective for dense prediction tasks.

  
